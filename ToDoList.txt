
-----------------------------------------------------------------------------------------------------------------------
ORDER 

1. Sophie get OpenLab on laptop tomorrow
2. Figure out variables for worker class, make sure they can be updated etc
    - add code to update these variables durign the crawl
3. Write variables out to a file at the very end
4. Figure out how we can detect similiar pages and implement iterate (hash?theshold?remove params?)
5. Figure how to get out of calender!
CRAWL CRAWL CRAWL FIND traps


-----------------------------------------------------------------------------------------------------------------------
TO DO

**** Keep track of necessary variables by using parameters and at the very end then write to a single file (iterate through each worker) ****

    VARIABLES IN WORKER (for each worker)
    DONE + mostWords[url,wordCount]
    DONE + number of unique pages
    DONE + 50 most common words in the entire set of pages crawled
    DONE + number of subdomains in ics.uci.edu Domain
-------------------------------------------------------------------

-links inside the text too

DONE + figure out how to keep track of variables

- avoid traps
    - figure out how to not go into those gajillion comments and replies to comments on evoke.ics.uci.edu

- detect similar documents
    - use python library 
    - similarity threshold to avoid falling into traps -> get a hash number for the webpage itself and compare??

- add find for urls in other places other than anchor tag (dont look at src attributes though)

- redirecting pages that actually refer to the same page - get that full link first???
    import urllib2
    response = urllib2.urlopen(HeadRequest("http://google.com/index.html"))
    response.geturl()

- figure how to get out of the calendar

- add domain and scheme if only given path??
    o = urlparse("")
    if o.scheme is empty .. must add scheme?? how to know when to add domain??


-----------------------------------------------------------------------------------------------------------------------
REPORT

1. # of unique pages?
    ----> length of visitedURL set 
        - add filter using similarity detector (hash?)
        - choosing to filter by removing paramters or by hash similarity 

2. Longest page in # of words
    ----> longestPage tuple(url,#)

        - have a maxWordCount and compare to current url's wordCount
        - set new maxWordCount if greater 

3. 50 most common words in the entire set of pages crawled?
    (ignore english stopwords)
    ----> dictionary (word:count)

        - add word/token into dictionary and increment count
        - at the end, sort and convert to list of tuples and get top 50 

4. How many subdomains in ics.uci.edu domain?
    (subdomain if name comes before ics.uci.edu)
    ----> dictionary (subdomain:count)

        - check if ____ comes before .ics.uci.edu

    EX: http://vision.ics.uci.edu, 10




-----------------------------------------------------------------------------------------------------------------------
LAST:
- test on the other 3 links then all four together